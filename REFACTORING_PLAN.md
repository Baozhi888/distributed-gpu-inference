# åˆ†å¸ƒå¼ GPU æ¨ç†å¹³å°é‡æ„å®æ–½è®¡åˆ’

åŸºäº Petals é¡¹ç›®æ€æƒ³ + distributed-gpu-inference é¡¹ç›®æ¶æ„ + 2024-2025 æœ€æ–°æŠ€æœ¯

---

## ä¸€ã€é¡¹ç›®å¯¹æ¯”åˆ†æ

### 1.1 Petals é¡¹ç›®æ ¸å¿ƒä»·å€¼ï¼ˆå€¼å¾—ä¿ç•™ï¼‰

| ç‰¹æ€§ | å®ç°æ–¹å¼ | ä»·å€¼è¯„ä¼° |
|------|----------|---------|
| **åˆ†å±‚æ¨¡å‹åˆ†å‰²** | å°†å¤§æ¨¡å‹æŒ‰ Transformer Block åˆ‡åˆ†åˆ°ä¸åŒèŠ‚ç‚¹ | â­â­â­â­â­ æ ¸å¿ƒåˆ›æ–° |
| **è·¨èŠ‚ç‚¹ KV-Cache** | æ¯ä¸ªèŠ‚ç‚¹ç»´æŠ¤è‡ªå·±è´Ÿè´£çš„å±‚çš„ KV-Cache | â­â­â­â­â­ å…³é”®æŠ€æœ¯ |
| **P2P èŠ‚ç‚¹å‘ç°** | åŸºäº Hivemind DHT çš„å»ä¸­å¿ƒåŒ–æœåŠ¡å‘ç° | â­â­â­ å¯ç”¨ä¸­å¿ƒåŒ–æ›¿ä»£ |
| **å®¹é”™è·¯ç”±** | èŠ‚ç‚¹æ•…éšœè‡ªåŠ¨åˆ‡æ¢ã€KV-Cache é‡å»º | â­â­â­â­ å¿…é¡»ä¿ç•™ |
| **æ¨æµ‹è§£ç ** | åŸºç¡€æ”¯æŒï¼ˆprimitives for speculative decodingï¼‰ | â­â­â­ éœ€è¦å¢å¼º |

### 1.2 Petals é¡¹ç›®å±€é™æ€§ï¼ˆéœ€è¦æ”¹è¿›ï¼‰

| é—®é¢˜ | å½±å“ | å»ºè®®æ–¹æ¡ˆ |
|------|------|---------|
| **ä¾èµ–è¿‡æ—¶çš„ Hivemind** | ç»´æŠ¤å›°éš¾ã€æ€§èƒ½å—é™ | ç”¨ç°ä»£ gRPC + Redis æ›¿ä»£ |
| **æ—  PagedAttention** | å†…å­˜åˆ©ç”¨ç‡ä½ï¼ˆä»… 20-38%ï¼‰ | é›†æˆ vLLM/SGLang åç«¯ |
| **æ—  Prefill/Decode åˆ†ç¦»** | æ— æ³•é’ˆå¯¹æ€§ä¼˜åŒ– | é‡‡ç”¨ DistServe æ¶æ„ |
| **Transformers ç‰ˆæœ¬é”å®š** | æ— æ³•ä½¿ç”¨æ–°æ¨¡å‹/æ–°ç‰¹æ€§ | è§£è€¦æ¨¡å‹å±‚ä¸æ¡†æ¶å±‚ |
| **å•ä¸€é‡åŒ–æ–¹æ¡ˆ** | INT8/NF4 ä¸å¤Ÿçµæ´» | æ”¯æŒ FP8/AWQ/GPTQ |

### 1.3 ä½ çš„ distributed-gpu-inference é¡¹ç›®ä¼˜åŠ¿

| ç‰¹æ€§ | ä¼˜åŠ¿ |
|------|------|
| **ç°ä»£æŠ€æœ¯æ ˆ** | FastAPI + PostgreSQL + Redisï¼Œæ˜“äºç»´æŠ¤ |
| **Worker å¯é æ€§è¯„åˆ†** | æ™ºèƒ½è°ƒåº¦ï¼Œè´¨é‡ä¿éšœ |
| **å¤šä»»åŠ¡ç±»å‹** | LLM/å›¾åƒ/è¯­éŸ³ï¼Œçµæ´»æ‰©å±• |
| **å®‰å…¨æœºåˆ¶å®Œå–„** | Token è½®æ¢ã€è¯·æ±‚ç­¾åã€è´¦æˆ·é”å®š |
| **ç”¨æˆ·ä½“éªŒå¥½** | npx ä¸€é”®å®‰è£…ï¼Œäº¤äº’å¼é…ç½® |

### 1.4 ä½ çš„é¡¹ç›®éœ€è¦å¢å¼ºçš„æ–¹é¢

| é—®é¢˜ | å½“å‰çŠ¶æ€ | ç›®æ ‡çŠ¶æ€ |
|------|----------|---------|
| **æ¨ç†æ•ˆç‡** | åŸç”Ÿ Transformers | vLLM/SGLang åç«¯ |
| **å¤§æ¨¡å‹æ”¯æŒ** | å• Worker å®Œæ•´æ¨¡å‹ | åˆ†å±‚åˆ†å¸ƒå¼æ¨ç† |
| **KV-Cache ç®¡ç†** | æ—  | PagedAttention + è·¨èŠ‚ç‚¹å…±äº« |
| **æ‰¹å¤„ç†ä¼˜åŒ–** | æ—  | è¿ç»­æ‰¹å¤„ç†(Continuous Batching) |

---

## äºŒã€æŠ€æœ¯é€‰å‹å»ºè®®

### 2.1 æ¨ç†åç«¯é€‰æ‹©

**æ¨èæ–¹æ¡ˆï¼šSGLang > vLLM > TensorRT-LLM**

| æ¡†æ¶ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|------|------|---------|
| **[SGLang](https://github.com/sgl-project/sglang)** | çº¯ Pythonã€<4K è¡Œæ ¸å¿ƒä»£ç ã€RadixAttention å‰ç¼€ç¼“å­˜ã€3.1x ååé‡ | ç”Ÿæ€ç•¥å° | é«˜ KV å¤ç”¨åœºæ™¯ï¼ˆRAGã€Agentï¼‰ |
| **[vLLM](https://github.com/vllm-project/vllm)** | ç”Ÿæ€æœ€å®Œå–„ã€PagedAttentionã€PyTorch åŸºé‡‘ä¼šé¡¹ç›® | ä»£ç å¤æ‚ | é€šç”¨ç”Ÿäº§ç¯å¢ƒ |
| **TensorRT-LLM** | NVIDIA æ·±åº¦ä¼˜åŒ–ã€B200 æœ€ä½³æ€§èƒ½ | ä»…æ”¯æŒ NVIDIAã€é…ç½®å¤æ‚ | çº¯ NVIDIA é«˜ç«¯é›†ç¾¤ |

**å»ºè®®**ï¼šä»¥ SGLang ä¸ºä¸»è¦åç«¯ï¼Œä¿ç•™ vLLM å…¼å®¹æ€§

### 2.2 åˆ†å¸ƒå¼æ¶æ„é€‰æ‹©

**æ¨èæ–¹æ¡ˆï¼šæ··åˆæ¶æ„ï¼ˆä¸­å¿ƒè°ƒåº¦ + P2P ç›´è¿ï¼‰**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ§åˆ¶å¹³é¢ (Control Plane)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  FastAPI    â”‚  â”‚  PostgreSQL â”‚  â”‚  Redis (çŠ¶æ€ + ç¼“å­˜)     â”‚  â”‚
â”‚  â”‚  è°ƒåº¦æœåŠ¡    â”‚  â”‚  å…ƒæ•°æ®å­˜å‚¨  â”‚  â”‚  KV-Cache ç´¢å¼•          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ æ§åˆ¶ä¿¡ä»¤
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                   â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Worker 1 â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚ Worker 2 â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚ Worker 3 â”‚
    â”‚ Prefill  â”‚  RDMA  â”‚ Decode   â”‚  gRPC  â”‚ Decode   â”‚
    â”‚ A100     â”‚  KVä¼ è¾“ â”‚ RTX 4090 â”‚  æ¿€æ´»å€¼ â”‚ RTX 3090 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          æ•°æ®å¹³é¢ (Data Plane) - P2P ç›´è¿
```

**å…³é”®æŠ€æœ¯å†³ç­–**ï¼š

1. **æ§åˆ¶å¹³é¢**ï¼šä¿ç•™ä½ ç°æœ‰çš„ FastAPI ä¸­å¿ƒåŒ–æ¶æ„
2. **æ•°æ®å¹³é¢**ï¼šWorker é—´ P2P ç›´è¿ï¼ˆgRPC Streamingï¼‰
3. **KV-Cache ä¼ è¾“**ï¼šæ”¯æŒ RDMAï¼ˆé«˜ç«¯ï¼‰/ TCPï¼ˆé€šç”¨ï¼‰

### 2.3 æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶

| ç»„ä»¶ | æŠ€æœ¯é€‰å‹ | å‚è€ƒå®ç° |
|------|---------|---------|
| æ¨¡å‹åˆ†å±‚ | æŒ‰ Transformer Block åˆ‡åˆ† | Petals `RemoteSequential` |
| KV-Cache ç®¡ç† | PagedAttention + åˆ†å±‚ç¼“å­˜ | [LMCache](https://github.com/LMCache/LMCache) |
| Prefill/Decode åˆ†ç¦» | ç‹¬ç«‹ Worker æ±  | [DistServe](https://hao-ai-lab.github.io/blogs/distserve/) |
| KV-Cache ä¼ è¾“ | RDMA/TCP | [Mooncake](https://github.com/kvcache-ai/Mooncake) |
| æ¨æµ‹è§£ç  | EAGLE-3 | [EAGLE](https://github.com/SafeAILab/EAGLE) |
| è´Ÿè½½å‡è¡¡ | Max-Flow ç®—æ³• | [Helix](https://dl.acm.org/doi/10.1145/3669940.3707215) |

---

## ä¸‰ã€é‡æ„å®æ–½è®¡åˆ’

### Phase 1: æ¨ç†åç«¯å‡çº§ï¼ˆ2-3 å‘¨ï¼‰

**ç›®æ ‡**ï¼šå°†å• Worker æ¨ç†æ•ˆç‡æå‡ 5-10x

#### 1.1 é›†æˆ SGLang/vLLM ä½œä¸ºæ¨ç†åç«¯

```python
# worker/engines/llm_optimized.py
from typing import Dict, Any
import sglang as sgl
from .base import BaseEngine

class OptimizedLLMEngine(BaseEngine):
    """åŸºäº SGLang çš„é«˜æ€§èƒ½æ¨ç†å¼•æ“"""

    def load_model(self) -> None:
        model_id = self.config.get("model_id")

        # SGLang Runtime è‡ªå¸¦ PagedAttention
        self.runtime = sgl.Runtime(
            model_path=model_id,
            tp_size=1,  # å• GPU å¼ é‡å¹¶è¡Œ
            mem_fraction_static=0.8,  # GPU å†…å­˜å ç”¨
            chunked_prefill_size=8192,  # åˆ†å—é¢„å¡«å……
        )

    def inference(self, params: Dict[str, Any]) -> Dict[str, Any]:
        messages = params.get("messages", [])

        # ä½¿ç”¨ SGLang çš„ RadixAttention å®ç°å‰ç¼€ç¼“å­˜
        state = self.runtime.generate(
            prompt=messages,
            sampling_params={
                "max_new_tokens": params.get("max_tokens", 2048),
                "temperature": params.get("temperature", 0.7),
            }
        )
        return {"response": state.text, "usage": state.usage}
```

#### 1.2 æ·»åŠ è¿ç»­æ‰¹å¤„ç†æ”¯æŒ

```python
# worker/batch_processor.py
import asyncio
from dataclasses import dataclass
from typing import List

@dataclass
class PendingRequest:
    job_id: str
    params: dict
    future: asyncio.Future

class ContinuousBatcher:
    """è¿ç»­æ‰¹å¤„ç†å™¨ - åŠ¨æ€åˆå¹¶è¯·æ±‚"""

    def __init__(self, engine, max_batch_size=32, max_wait_ms=50):
        self.engine = engine
        self.max_batch_size = max_batch_size
        self.max_wait_ms = max_wait_ms
        self.pending: List[PendingRequest] = []
        self._batch_task = None

    async def submit(self, job_id: str, params: dict) -> dict:
        future = asyncio.Future()
        self.pending.append(PendingRequest(job_id, params, future))

        if len(self.pending) >= self.max_batch_size:
            await self._process_batch()
        elif self._batch_task is None:
            self._batch_task = asyncio.create_task(self._wait_and_process())

        return await future

    async def _wait_and_process(self):
        await asyncio.sleep(self.max_wait_ms / 1000)
        await self._process_batch()

    async def _process_batch(self):
        if not self.pending:
            return

        batch = self.pending[:self.max_batch_size]
        self.pending = self.pending[self.max_batch_size:]

        # æ‰¹é‡æ¨ç†
        results = await self.engine.batch_inference([r.params for r in batch])

        for req, result in zip(batch, results):
            req.future.set_result(result)
```

#### 1.3 ä»»åŠ¡æ¸…å•

- [x] æ·»åŠ  SGLang ä½œä¸ºå¯é€‰æ¨ç†åç«¯ âœ… `worker/engines/llm_sglang.py`
- [x] å®ç°è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰âœ… `worker/batch_processor.py`
- [x] æ·»åŠ å‰ç¼€ç¼“å­˜ï¼ˆPrefix Cachingï¼‰æ”¯æŒ âœ… é›†æˆåœ¨ SGLang/vLLM å¼•æ“ä¸­
- [ ] åŸºå‡†æµ‹è¯•ï¼šå¯¹æ¯”åŸç”Ÿ Transformers vs SGLang

---

### Phase 2: åˆ†å¸ƒå¼æ¨¡å‹åˆ†å‰²ï¼ˆ3-4 å‘¨ï¼‰

**ç›®æ ‡**ï¼šæ”¯æŒè¶…è¿‡å•å¡æ˜¾å­˜çš„å¤§æ¨¡å‹

#### 2.1 æ¨¡å‹åˆ†å±‚æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Llama-70B (80å±‚)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Worker 1 (A100 80GB)    â”‚  Layer 0-26  â”‚  Prefill ä¸“ç”¨    â”‚
â”‚  Worker 2 (RTX 4090 24GB)â”‚  Layer 27-53 â”‚  Decode ä¸“ç”¨     â”‚
â”‚  Worker 3 (RTX 4090 24GB)â”‚  Layer 54-79 â”‚  Decode ä¸“ç”¨     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.2 æ ¸å¿ƒæ•°æ®ç»“æ„

```python
# common/data_structures.py
from dataclasses import dataclass
from typing import List, Optional
import torch

@dataclass
class BlockRange:
    """æ¨¡å‹å±‚èŒƒå›´"""
    start: int  # èµ·å§‹å±‚ï¼ˆåŒ…å«ï¼‰
    end: int    # ç»“æŸå±‚ï¼ˆä¸åŒ…å«ï¼‰

@dataclass
class WorkerInfo:
    """Worker ä¿¡æ¯"""
    worker_id: str
    blocks: BlockRange          # è´Ÿè´£çš„å±‚èŒƒå›´
    role: str                   # "prefill" | "decode" | "hybrid"
    gpu_memory_gb: float
    peer_address: str           # P2P ç›´è¿åœ°å€

@dataclass
class InferenceState:
    """æ¨ç†çŠ¶æ€ï¼ˆè·¨ Worker ä¼ é€’ï¼‰"""
    session_id: str
    hidden_states: torch.Tensor     # [batch, seq_len, hidden_dim]
    position: int                   # å½“å‰ä½ç½®
    kv_cache_keys: List[str]        # KV-Cache åœ¨å„ Worker çš„ç´¢å¼•

@dataclass
class KVCacheBlock:
    """KV-Cache åˆ†é¡µå—"""
    block_id: str
    layer_idx: int
    keys: torch.Tensor      # [num_heads, block_size, head_dim]
    values: torch.Tensor
    ref_count: int = 1      # å¼•ç”¨è®¡æ•°ï¼ˆCopy-on-Writeï¼‰
```

#### 2.3 åˆ†å¸ƒå¼æ¨ç†ä¼šè¯

```python
# client/distributed_session.py
from typing import List
import grpc
import torch

class DistributedInferenceSession:
    """åˆ†å¸ƒå¼æ¨ç†ä¼šè¯ - å‚è€ƒ Petals InferenceSession"""

    def __init__(self, scheduler, model_name: str, max_length: int = 4096):
        self.scheduler = scheduler
        self.model_name = model_name
        self.max_length = max_length
        self.worker_sessions: List[WorkerSession] = []
        self.position = 0

    async def setup(self):
        """å»ºç«‹åˆ°å„ Worker çš„è¿æ¥"""
        # ä»è°ƒåº¦å™¨è·å–è·¯ç”±è®¡åˆ’
        route = await self.scheduler.get_inference_route(
            self.model_name,
            self.max_length
        )

        # å»ºç«‹ P2P è¿æ¥
        for worker_info in route.workers:
            session = await WorkerSession.connect(
                worker_info.peer_address,
                worker_info.blocks
            )
            self.worker_sessions.append(session)

        # é“¾æ¥å„ä¼šè¯ï¼ˆç”¨äº server-to-server ä¼ è¾“ï¼‰
        for i in range(len(self.worker_sessions) - 1):
            self.worker_sessions[i].next_session = self.worker_sessions[i + 1]

    async def step(self, inputs: torch.Tensor) -> torch.Tensor:
        """æ‰§è¡Œä¸€æ­¥æ¨ç†"""
        hidden_states = inputs

        for session in self.worker_sessions:
            try:
                hidden_states = await session.forward(
                    hidden_states,
                    position=self.position
                )
            except Exception as e:
                # å®¹é”™ï¼šé‡æ–°è·¯ç”±
                await self._handle_failure(session, e)
                hidden_states = await session.forward(hidden_states, self.position)

        self.position += inputs.shape[1]
        return hidden_states

    async def _handle_failure(self, failed_session: WorkerSession, error: Exception):
        """å¤„ç† Worker æ•…éšœ"""
        # 1. æŠ¥å‘Šæ•…éšœ
        await self.scheduler.report_failure(failed_session.worker_id, error)

        # 2. è·å–æ›¿ä»£è·¯ç”±
        new_route = await self.scheduler.get_alternative_route(
            failed_session.blocks,
            exclude=[failed_session.worker_id]
        )

        # 3. é‡å»ºä¼šè¯ï¼ˆéœ€è¦é‡æ–°è®¡ç®— KV-Cacheï¼‰
        new_session = await WorkerSession.connect(
            new_route.peer_address,
            new_route.blocks
        )

        # 4. æ›¿æ¢æ•…éšœä¼šè¯
        idx = self.worker_sessions.index(failed_session)
        self.worker_sessions[idx] = new_session
```

#### 2.4 Worker é—´ gRPC é€šä¿¡

```protobuf
// proto/inference.proto
syntax = "proto3";

service DistributedInference {
    // æµå¼æ¨ç†ï¼ˆæ”¯æŒ server-to-server è½¬å‘ï¼‰
    rpc StreamInference(stream InferenceRequest) returns (stream InferenceResponse);

    // KV-Cache ä¼ è¾“
    rpc TransferKVCache(KVCacheRequest) returns (KVCacheResponse);
}

message InferenceRequest {
    string session_id = 1;
    bytes hidden_states = 2;      // åºåˆ—åŒ–çš„ Tensor
    int32 position = 3;
    repeated string kv_cache_keys = 4;

    // å¯é€‰ï¼šä¸‹ä¸€è·³ä¿¡æ¯ï¼ˆç”¨äº server-to-serverï¼‰
    string next_worker_address = 5;
}

message InferenceResponse {
    bytes hidden_states = 1;
    repeated string updated_kv_keys = 2;
    int64 latency_ms = 3;
}
```

#### 2.5 ä»»åŠ¡æ¸…å•

- [x] å®šä¹‰åˆ†å¸ƒå¼æ•°æ®ç»“æ„ âœ… `common/data_structures.py`
- [x] å®ç°æ¨¡å‹åˆ†å±‚åŠ è½½ï¼ˆæŒ‰ Block èŒƒå›´ï¼‰âœ… `worker/distributed/model_shard.py`
- [x] å®ç° Worker é—´ gRPC Streaming é€šä¿¡ âœ… `worker/distributed/grpc_server.py`, `proto/inference.proto`
- [x] å®ç°åˆ†å¸ƒå¼æ¨ç†ä¼šè¯ç®¡ç† âœ… `worker/distributed/session.py`
- [x] å®ç°æ•…éšœæ£€æµ‹ä¸è‡ªåŠ¨æ¢å¤ âœ… é›†æˆåœ¨ `DistributedInferenceSession` ä¸­
- [ ] åŸºå‡†æµ‹è¯•ï¼šLlama-70B è·¨ 3 å¡æ¨ç†

---

### Phase 3: KV-Cache åˆ†å¸ƒå¼ç®¡ç†ï¼ˆ2-3 å‘¨ï¼‰

**ç›®æ ‡**ï¼šé«˜æ•ˆçš„è·¨èŠ‚ç‚¹ KV-Cache å…±äº«ä¸ä¼ è¾“

#### 3.1 åˆ†å±‚ç¼“å­˜æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     KV-Cache åˆ†å±‚å­˜å‚¨                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  L1: GPU HBM      â”‚  æœ€çƒ­æ•°æ®  â”‚  <1ms å»¶è¿Ÿ   â”‚  PagedAttention â”‚
â”‚  L2: CPU RAM      â”‚  æ¸©æ•°æ®    â”‚  ~5ms å»¶è¿Ÿ   â”‚  å†…å­˜æ±           â”‚
â”‚  L3: Redis/NVMe   â”‚  å†·æ•°æ®    â”‚  ~10ms å»¶è¿Ÿ  â”‚  æŒä¹…åŒ–          â”‚
â”‚  L4: è¿œç¨‹ Worker   â”‚  å…±äº«å‰ç¼€  â”‚  ~50ms å»¶è¿Ÿ  â”‚  RDMA/TCP       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.2 KV-Cache ç®¡ç†å™¨

```python
# worker/kv_cache_manager.py
from typing import Dict, Optional
import torch
import hashlib

class DistributedKVCacheManager:
    """åˆ†å¸ƒå¼ KV-Cache ç®¡ç†å™¨"""

    def __init__(self,
                 gpu_cache_size_gb: float = 4.0,
                 cpu_cache_size_gb: float = 16.0,
                 redis_client = None):
        self.gpu_cache = PagedKVCache(size_gb=gpu_cache_size_gb)
        self.cpu_cache = CPUKVCache(size_gb=cpu_cache_size_gb)
        self.redis = redis_client
        self.block_table: Dict[str, str] = {}  # key -> location

    def get_prefix_key(self, tokens: torch.Tensor) -> str:
        """è®¡ç®—å‰ç¼€å“ˆå¸Œï¼ˆç”¨äºå…±äº«ï¼‰"""
        return hashlib.sha256(tokens.numpy().tobytes()).hexdigest()[:16]

    async def get_or_compute(self,
                             prefix_key: str,
                             compute_fn,
                             layer_idx: int) -> torch.Tensor:
        """è·å–æˆ–è®¡ç®— KV-Cache"""

        # L1: GPU ç¼“å­˜
        if prefix_key in self.gpu_cache:
            return self.gpu_cache.get(prefix_key, layer_idx)

        # L2: CPU ç¼“å­˜
        if prefix_key in self.cpu_cache:
            kv = self.cpu_cache.get(prefix_key, layer_idx)
            self.gpu_cache.put(prefix_key, layer_idx, kv)  # æå‡
            return kv

        # L3: Redisï¼ˆè·¨ Worker å…±äº«ï¼‰
        if self.redis:
            kv_bytes = await self.redis.get(f"kv:{prefix_key}:{layer_idx}")
            if kv_bytes:
                kv = self._deserialize(kv_bytes)
                self.gpu_cache.put(prefix_key, layer_idx, kv)
                return kv

        # L4: è®¡ç®—æ–°å€¼
        kv = await compute_fn()
        self.gpu_cache.put(prefix_key, layer_idx, kv)

        # å¼‚æ­¥å†™å›
        asyncio.create_task(self._write_back(prefix_key, layer_idx, kv))

        return kv

    async def transfer_to_peer(self,
                               prefix_key: str,
                               peer_address: str,
                               layer_range: BlockRange):
        """å°† KV-Cache ä¼ è¾“åˆ°å…¶ä»– Worker"""
        kv_data = []
        for layer_idx in range(layer_range.start, layer_range.end):
            kv = self.gpu_cache.get(prefix_key, layer_idx)
            kv_data.append(self._serialize(kv))

        # ä½¿ç”¨ gRPC ä¼ è¾“
        async with grpc.aio.insecure_channel(peer_address) as channel:
            stub = DistributedInferenceStub(channel)
            await stub.TransferKVCache(KVCacheRequest(
                prefix_key=prefix_key,
                layer_range=layer_range,
                kv_data=kv_data
            ))
```

#### 3.3 å‚è€ƒï¼šMooncake KV ä¼ è¾“ä¼˜åŒ–

```python
# é«˜æ€§èƒ½ KV ä¼ è¾“ï¼ˆå‚è€ƒ Mooncakeï¼‰
class RDMAKVTransfer:
    """RDMA ç›´æ¥å†…å­˜è®¿é—®ï¼ˆé«˜ç«¯åœºæ™¯ï¼‰"""

    def __init__(self, device_name: str = "mlx5_0"):
        # åˆå§‹åŒ– RDMA èµ„æº
        self.ctx = rdma.Context(device_name)
        self.pd = rdma.ProtectionDomain(self.ctx)

    async def zero_copy_transfer(self,
                                 local_tensor: torch.Tensor,
                                 remote_addr: str) -> None:
        """é›¶æ‹·è´ä¼ è¾“ - ç›´æ¥ GPU-to-GPU"""
        # æ³¨å†Œå†…å­˜åŒºåŸŸ
        mr = self.pd.register_mr(
            local_tensor.data_ptr(),
            local_tensor.nbytes,
            rdma.IBV_ACCESS_LOCAL_WRITE | rdma.IBV_ACCESS_REMOTE_READ
        )

        # RDMA Write
        await self._rdma_write(mr, remote_addr)
```

#### 3.4 ä»»åŠ¡æ¸…å•

- [x] å®ç° PagedAttention é£æ ¼çš„ KV-Cache ç®¡ç† âœ… `worker/distributed/kv_cache.py`
- [x] å®ç°åˆ†å±‚ç¼“å­˜ï¼ˆGPU â†’ CPU â†’ Redisï¼‰âœ… `DistributedKVCacheManager`
- [x] å®ç°å‰ç¼€å…±äº«ï¼ˆRadixAttention æ€æƒ³ï¼‰âœ… `compute_prefix_hash()` å‡½æ•°
- [x] å®ç°è·¨ Worker KV-Cache ä¼ è¾“ âœ… `TransferKVCache` gRPC æ–¹æ³•
- [ ] å¯é€‰ï¼šRDMA é›¶æ‹·è´ä¼˜åŒ–

---

### Phase 4: Prefill/Decode åˆ†ç¦»ï¼ˆ2-3 å‘¨ï¼‰

**ç›®æ ‡**ï¼šä¼˜åŒ– TTFTï¼ˆé¦– Token æ—¶é—´ï¼‰å’Œååé‡

#### 4.1 æ¶æ„è®¾è®¡ï¼ˆå‚è€ƒ DistServeï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Prefill/Decode åˆ†ç¦»æ¶æ„                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  Prefill Pool   â”‚         â”‚   Decode Pool   â”‚          â”‚
â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  KVä¼ è¾“  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚          â”‚
â”‚   â”‚  - A100 x 2     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  - RTX 4090 x 4 â”‚          â”‚
â”‚   â”‚  - å¤§æ‰¹é‡å¤„ç†    â”‚         â”‚  - ä½å»¶è¿Ÿè§£ç     â”‚          â”‚
â”‚   â”‚  - è®¡ç®—å¯†é›†     â”‚         â”‚  - å†…å­˜å¯†é›†     â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                             â”‚
â”‚   ç‰¹ç‚¹ï¼š                                                     â”‚
â”‚   - Prefill: é«˜å¹¶è¡Œåº¦ï¼Œå……åˆ†åˆ©ç”¨ Tensor Core               â”‚
â”‚   - Decode:  ä½å»¶è¿Ÿï¼Œå……åˆ†åˆ©ç”¨æ˜¾å­˜å¸¦å®½                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.2 è°ƒåº¦å™¨å¢å¼º

```python
# server/services/pd_scheduler.py
from enum import Enum
from typing import List

class WorkerRole(Enum):
    PREFILL = "prefill"
    DECODE = "decode"
    HYBRID = "hybrid"

class PrefillDecodeScheduler:
    """Prefill/Decode åˆ†ç¦»è°ƒåº¦å™¨"""

    def __init__(self, db_session, redis_client):
        self.db = db_session
        self.redis = redis_client

    async def assign_job(self, job: Job) -> WorkerAssignment:
        """æ™ºèƒ½åˆ†é…ä»»åŠ¡"""

        if job.phase == "prefill":
            # Prefill é˜¶æ®µï¼šé€‰æ‹©è®¡ç®—èƒ½åŠ›å¼ºçš„ Worker
            workers = await self._get_workers_by_role(WorkerRole.PREFILL)
            selected = self._select_by_compute_capability(workers)

        elif job.phase == "decode":
            # Decode é˜¶æ®µï¼šé€‰æ‹©å†…å­˜å¸¦å®½é«˜ã€å»¶è¿Ÿä½çš„ Worker
            workers = await self._get_workers_by_role(WorkerRole.DECODE)
            selected = self._select_by_memory_bandwidth(workers)

            # ç¡®ä¿ KV-Cache å¯è¾¾
            if not await self._check_kv_availability(job.kv_cache_key, selected):
                # è§¦å‘ KV-Cache è¿ç§»
                await self._migrate_kv_cache(job.kv_cache_key, selected)

        return WorkerAssignment(worker=selected, phase=job.phase)

    async def _migrate_kv_cache(self, kv_key: str, target_worker: Worker):
        """è¿ç§» KV-Cache åˆ°ç›®æ ‡ Worker"""
        # æ‰¾åˆ°å½“å‰æŒæœ‰ KV çš„ Worker
        source_worker = await self._find_kv_holder(kv_key)

        # å‘èµ·ä¼ è¾“
        await self._transfer_kv(
            source=source_worker,
            target=target_worker,
            kv_key=kv_key
        )
```

#### 4.3 ä»»åŠ¡æ¸…å•

- [x] æ‰©å±• Worker è§’è‰²ï¼ˆprefill/decode/hybridï¼‰âœ… `server/app/services/pd_scheduler.py`
- [x] å®ç° Prefill â†’ Decode çš„ KV-Cache ä¼ è¾“ âœ… `KVCacheMigrator`
- [x] è°ƒåº¦å™¨æ”¯æŒæŒ‰è§’è‰²åˆ†é… âœ… `PrefillDecodeScheduler`
- [x] å®ç° Prefill æ‰¹é‡åˆå¹¶ä¼˜åŒ– âœ… é›†æˆåœ¨è°ƒåº¦å™¨ä¸­
- [ ] åŸºå‡†æµ‹è¯•ï¼šTTFT å’Œååé‡æå‡

---

### Phase 5: æ¨æµ‹è§£ç é›†æˆï¼ˆ2-3 å‘¨ï¼‰

**ç›®æ ‡**ï¼šå•è¯·æ±‚è§£ç é€Ÿåº¦æå‡ 2-3x

#### 5.1 EAGLE-3 é›†æˆ

```python
# worker/engines/speculative.py
from typing import List, Tuple
import torch

class EAGLESpeculativeDecoder:
    """EAGLE-3 æ¨æµ‹è§£ç å™¨"""

    def __init__(self,
                 target_model,
                 draft_head,  # è½»é‡çº§é¢„æµ‹å¤´
                 tree_size: int = 60):
        self.target = target_model
        self.draft = draft_head
        self.tree_size = tree_size

    async def decode_step(self,
                          hidden_states: torch.Tensor,
                          kv_cache) -> Tuple[torch.Tensor, int]:
        """ä¸€æ­¥æ¨æµ‹è§£ç """

        # 1. Draft: ç”Ÿæˆå€™é€‰ token æ ‘
        draft_tokens, draft_tree = self._generate_draft_tree(
            hidden_states,
            tree_size=self.tree_size
        )

        # 2. Verify: ç›®æ ‡æ¨¡å‹å¹¶è¡ŒéªŒè¯
        with torch.no_grad():
            logits = self.target.forward(
                draft_tokens,
                attention_mask=self._build_tree_attention_mask(draft_tree),
                use_cache=True,
                past_key_values=kv_cache
            )

        # 3. Accept: ç¡®å®šæ¥å—çš„æœ€é•¿å‰ç¼€
        accepted_tokens, accepted_length = self._tree_verify(
            draft_tree,
            logits
        )

        return accepted_tokens, accepted_length

    def _generate_draft_tree(self, hidden_states, tree_size):
        """ç”Ÿæˆ token æ ‘ï¼ˆEAGLE ç‰¹æœ‰ï¼‰"""
        # EAGLE åœ¨ feature level è¿›è¡Œè‡ªå›å½’
        feature_draft = self.draft.forward(hidden_states)

        # ä½¿ç”¨ç›®æ ‡æ¨¡å‹çš„ LM head è·å– token
        logits = self.target.lm_head(feature_draft)

        # æ„å»ºæ ‘å½¢ç»“æ„
        tree = self._build_token_tree(logits, tree_size)
        return tree.tokens, tree
```

#### 5.2 ä¸åˆ†å¸ƒå¼æ¶æ„é›†æˆ

```python
# æ¨æµ‹è§£ç  + åˆ†å¸ƒå¼æ¨ç†
class DistributedSpeculativeSession(DistributedInferenceSession):
    """æ”¯æŒæ¨æµ‹è§£ç çš„åˆ†å¸ƒå¼ä¼šè¯"""

    async def speculative_step(self, inputs: torch.Tensor) -> torch.Tensor:
        """æ¨æµ‹è§£ç æ­¥éª¤"""

        # 1. åœ¨æœ€åä¸€ä¸ª Worker ä¸Šè¿è¡Œ draft
        last_worker = self.worker_sessions[-1]
        draft_tokens = await last_worker.generate_draft(inputs)

        # 2. å…¨æµæ°´çº¿éªŒè¯
        hidden_states = inputs
        for session in self.worker_sessions:
            hidden_states = await session.forward_with_draft(
                hidden_states,
                draft_tokens
            )

        # 3. ç¡®å®šæ¥å—é•¿åº¦
        accepted_length = await last_worker.verify_and_accept(hidden_states)

        self.position += accepted_length
        return hidden_states[:, :accepted_length]
```

#### 5.3 ä»»åŠ¡æ¸…å•

- [x] é›†æˆ EAGLE-3 æ¨æµ‹è§£ç å¤´ âœ… `worker/engines/speculative.py`
- [x] å®ç° Tree Attention æœºåˆ¶ âœ… `TreeDraftBuffer`
- [x] ä¸åˆ†å¸ƒå¼æ¨ç†æµæ°´çº¿é›†æˆ âœ… `SpeculativeDecoder`
- [x] æ”¯æŒåŠ¨æ€è°ƒæ•´æ¨æµ‹æ·±åº¦ âœ… `_adapt_depth()` æ–¹æ³•
- [ ] åŸºå‡†æµ‹è¯•ï¼šå•è¯·æ±‚å»¶è¿Ÿé™ä½

---

### Phase 6: ç”Ÿäº§åŒ–ä¸ä¼˜åŒ–ï¼ˆ2-3 å‘¨ï¼‰

#### 6.1 å¯è§‚æµ‹æ€§å¢å¼º

```python
# server/services/observability.py
from opentelemetry import trace, metrics
from prometheus_client import Counter, Histogram, Gauge

# Metrics
INFERENCE_LATENCY = Histogram(
    "inference_latency_seconds",
    "Inference latency",
    ["model", "phase", "worker_role"]
)

KV_CACHE_HIT_RATE = Gauge(
    "kv_cache_hit_rate",
    "KV cache hit rate",
    ["level"]  # gpu, cpu, redis, remote
)

TOKENS_PER_SECOND = Counter(
    "tokens_generated_total",
    "Total tokens generated",
    ["model", "worker_id"]
)

# Tracing
tracer = trace.get_tracer("distributed-inference")

async def traced_inference(session, inputs):
    with tracer.start_as_current_span("inference") as span:
        span.set_attribute("session_id", session.session_id)
        span.set_attribute("input_length", inputs.shape[1])

        result = await session.step(inputs)

        span.set_attribute("output_length", result.shape[1])
        return result
```

#### 6.2 éƒ¨ç½²æ¶æ„

```yaml
# docker-compose.production.yml
version: '3.8'

services:
  # æ§åˆ¶å¹³é¢
  api-server:
    image: distributed-inference/server:latest
    deploy:
      replicas: 2
    environment:
      - DATABASE_URL=postgresql://...
      - REDIS_URL=redis://...
    ports:
      - "8000:8000"

  # æ•°æ®å­˜å‚¨
  postgres:
    image: postgres:15
    volumes:
      - pgdata:/var/lib/postgresql/data

  redis:
    image: redis:7
    command: redis-server --maxmemory 16gb --maxmemory-policy allkeys-lru

  # ç›‘æ§
  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"

# Worker å•ç‹¬éƒ¨ç½²ï¼ˆå„ GPU æœºå™¨ï¼‰
# docker run -d --gpus all distributed-inference/worker:latest
```

#### 6.3 ä»»åŠ¡æ¸…å•

- [x] é›†æˆ OpenTelemetry åˆ†å¸ƒå¼è¿½è¸ª âœ… `server/app/services/observability.py`
- [x] æ·»åŠ  Prometheus æŒ‡æ ‡å¯¼å‡º âœ… `MetricsCollector` ç±»
- [ ] åˆ›å»º Grafana ç›‘æ§é¢æ¿
- [ ] ç¼–å†™ç”Ÿäº§éƒ¨ç½²æ–‡æ¡£
- [ ] æ€§èƒ½å‹æµ‹ä¸è°ƒä¼˜

---

## å››ã€æŠ€æœ¯å‚è€ƒèµ„æº

### è®ºæ–‡

| è®ºæ–‡ | æ ¸å¿ƒè´¡çŒ® | é“¾æ¥ |
|------|----------|------|
| **DistServe** | Prefill/Decode åˆ†ç¦»æ¶æ„ | [Hao AI Lab](https://hao-ai-lab.github.io/blogs/distserve/) |
| **PagedAttention** | KV-Cache å†…å­˜ç®¡ç† | [arXiv:2309.06180](https://arxiv.org/abs/2309.06180) |
| **EAGLE-3** | é«˜æ•ˆæ¨æµ‹è§£ç  | [arXiv:2503.01840](https://arxiv.org/abs/2503.01840) |
| **Helix** | å¼‚æ„ GPU Max-Flow è°ƒåº¦ | [ACM ASPLOS 2025](https://dl.acm.org/doi/10.1145/3669940.3707215) |
| **Mooncake** | KV-Cache ä¼ è¾“ä¼˜åŒ– | [FAST 2025 Best Paper](https://github.com/kvcache-ai/Mooncake) |
| **FlowKV** | RDMA KV ä¼ è¾“ | [arXiv:2504.03775](https://arxiv.org/abs/2504.03775) |

### å¼€æºé¡¹ç›®

| é¡¹ç›® | ç”¨é€” | é“¾æ¥ |
|------|------|------|
| **SGLang** | é«˜æ€§èƒ½æ¨ç†åç«¯ | [github.com/sgl-project/sglang](https://github.com/sgl-project/sglang) |
| **vLLM** | æ¨ç†å¼•æ“ | [github.com/vllm-project/vllm](https://github.com/vllm-project/vllm) |
| **LMCache** | KV-Cache ç®¡ç† | [github.com/LMCache/LMCache](https://github.com/LMCache/LMCache) |
| **EAGLE** | æ¨æµ‹è§£ç  | [github.com/SafeAILab/EAGLE](https://github.com/SafeAILab/EAGLE) |
| **Petals** | åˆ†å¸ƒå¼æ¨ç†å‚è€ƒ | [github.com/bigscience-workshop/petals](https://github.com/bigscience-workshop/petals) |

---

## äº”ã€å»ºè®®çš„é¡¹ç›®ç»“æ„

```
distributed-gpu-inference/
â”œâ”€â”€ server/                         # æ§åˆ¶å¹³é¢
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ jobs.py             # ä»»åŠ¡ API
â”‚   â”‚   â”‚   â”œâ”€â”€ workers.py          # Worker API
â”‚   â”‚   â”‚   â””â”€â”€ inference.py        # åˆ†å¸ƒå¼æ¨ç† API (æ–°å¢)
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ scheduler.py        # æ™ºèƒ½è°ƒåº¦
â”‚   â”‚   â”‚   â”œâ”€â”€ pd_scheduler.py     # Prefill/Decode è°ƒåº¦ (æ–°å¢)
â”‚   â”‚   â”‚   â”œâ”€â”€ route_planner.py    # è·¯ç”±è§„åˆ’ (æ–°å¢)
â”‚   â”‚   â”‚   â””â”€â”€ kv_index.py         # KV-Cache ç´¢å¼• (æ–°å¢)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ worker/                         # æ•°æ®å¹³é¢
â”‚   â”œâ”€â”€ engines/
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ llm.py                  # åŸç”Ÿæ¨ç† (ä¿ç•™)
â”‚   â”‚   â”œâ”€â”€ llm_sglang.py           # SGLang åç«¯ (æ–°å¢)
â”‚   â”‚   â”œâ”€â”€ llm_vllm.py             # vLLM åç«¯ (æ–°å¢)
â”‚   â”‚   â””â”€â”€ speculative.py          # æ¨æµ‹è§£ç  (æ–°å¢)
â”‚   â”œâ”€â”€ distributed/                # åˆ†å¸ƒå¼ç»„ä»¶ (æ–°å¢)
â”‚   â”‚   â”œâ”€â”€ session.py              # åˆ†å¸ƒå¼ä¼šè¯
â”‚   â”‚   â”œâ”€â”€ kv_cache.py             # KV-Cache ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ grpc_server.py          # P2P é€šä¿¡
â”‚   â”‚   â””â”€â”€ model_shard.py          # æ¨¡å‹åˆ†ç‰‡
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ proto/                          # gRPC åè®® (æ–°å¢)
â”‚   â””â”€â”€ inference.proto
â”‚
â”œâ”€â”€ common/                         # å…±äº«ç»„ä»¶ (æ–°å¢)
â”‚   â”œâ”€â”€ data_structures.py
â”‚   â””â”€â”€ serialization.py
â”‚
â””â”€â”€ benchmarks/                     # æ€§èƒ½æµ‹è¯• (æ–°å¢)
    â”œâ”€â”€ single_worker.py
    â”œâ”€â”€ distributed.py
    â””â”€â”€ speculative.py
```

---

## å…­ã€é£é™©ä¸ç¼“è§£

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|---------|
| KV-Cache ä¼ è¾“å»¶è¿Ÿ | å½±å“ç«¯åˆ°ç«¯å»¶è¿Ÿ | 1. å°±è¿‘è°ƒåº¦ 2. é¢„å– 3. RDMA |
| Worker å¼‚æ„æ€§ | éš¾ä»¥ç»Ÿä¸€ç®¡ç† | 1. æŠ½è±¡å¼•æ“æ¥å£ 2. æŒ‰èƒ½åŠ›åˆ†ç±» |
| æ¨¡å‹å…¼å®¹æ€§ | æ–°æ¨¡å‹æ”¯æŒå›°éš¾ | 1. ä¾èµ– SGLang/vLLM 2. æ¨¡å—åŒ–è®¾è®¡ |
| å¤æ‚åº¦å¢åŠ  | å¼€å‘ç»´æŠ¤å›°éš¾ | 1. åˆ†é˜¶æ®µå®æ–½ 2. å……åˆ†æµ‹è¯• |

---

## ä¸ƒã€æ€»ç»“å»ºè®®

**å¼ºçƒˆå»ºè®®åŸºäºä½ çš„ `distributed-gpu-inference` é¡¹ç›®è¿›è¡Œé‡æ„**ï¼ŒåŸå› ï¼š

1. **ç°ä»£æŠ€æœ¯æ ˆ**ï¼šFastAPI + PostgreSQL æ¯” Petals çš„ Hivemind æ›´æ˜“ç»´æŠ¤
2. **å·²æœ‰åŸºç¡€**ï¼šWorker ç®¡ç†ã€å¯é æ€§è¯„åˆ†ã€å®‰å…¨æœºåˆ¶éƒ½å·²å®Œå–„
3. **çµæ´»æ¶æ„**ï¼šä¸­å¿ƒåŒ–è°ƒåº¦ + P2P ç›´è¿çš„æ··åˆæ¶æ„æ›´å®ç”¨
4. **å­¦ä¹  Petals ç²¾å**ï¼šåˆ†å±‚æ¨¡å‹ã€KV-Cache ç®¡ç†ã€å®¹é”™è·¯ç”±

**å®æ–½ä¼˜å…ˆçº§**ï¼š

1. â­â­â­â­â­ Phase 1ï¼ˆæ¨ç†åç«¯å‡çº§ï¼‰- ç«‹ç«¿è§å½±çš„æ€§èƒ½æå‡
2. â­â­â­â­ Phase 3ï¼ˆKV-Cache ç®¡ç†ï¼‰- å¤šç”¨æˆ·åœºæ™¯å¿…éœ€
3. â­â­â­â­ Phase 2ï¼ˆåˆ†å¸ƒå¼æ¨¡å‹åˆ†å‰²ï¼‰- å¤§æ¨¡å‹å¿…éœ€
4. â­â­â­ Phase 4ï¼ˆPrefill/Decode åˆ†ç¦»ï¼‰- é«˜çº§ä¼˜åŒ–
5. â­â­â­ Phase 5ï¼ˆæ¨æµ‹è§£ç ï¼‰- å»¶è¿Ÿæ•æ„Ÿåœºæ™¯

---

## å…«ã€å¾…å®Œæˆä»»åŠ¡æ¸…å•ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

> æ”¶é›†è‡ªå„é˜¶æ®µä»»åŠ¡æ¸…å•ä¸­çš„æœªå®Œæˆé¡¹ï¼ŒæŒ‰å®æ–½ä¼˜å…ˆçº§å’Œä¾èµ–å…³ç³»æ’åº

### ğŸ”´ P0 - é«˜ä¼˜å…ˆçº§ï¼ˆéªŒè¯æ ¸å¿ƒåŠŸèƒ½ï¼‰

| åºå· | ä»»åŠ¡ | æ¥æº | è¯´æ˜ | é¢„è®¡å·¥æ—¶ | çŠ¶æ€ |
|------|------|------|------|----------|------|
| 1 | åŸºå‡†æµ‹è¯•ï¼šå¯¹æ¯”åŸç”Ÿ Transformers vs SGLang | Phase 1 | éªŒè¯æ¨ç†åç«¯å‡çº§æ•ˆæœï¼Œé‡åŒ–æ€§èƒ½æå‡ | 1-2 å¤© | âœ… è„šæœ¬å·²å®Œæˆ |
| 2 | åŸºå‡†æµ‹è¯•ï¼šLlama-70B è·¨ 3 å¡æ¨ç† | Phase 2 | éªŒè¯åˆ†å¸ƒå¼æ¨¡å‹åˆ†å‰²åŠŸèƒ½ï¼Œç¡®è®¤å¤§æ¨¡å‹æ”¯æŒ | 2-3 å¤© | âœ… è„šæœ¬å·²å®Œæˆ |
| 3 | åŸºå‡†æµ‹è¯•ï¼šTTFT å’Œååé‡æå‡ | Phase 4 | éªŒè¯ Prefill/Decode åˆ†ç¦»æ•ˆæœ | 1-2 å¤© | âœ… è„šæœ¬å·²å®Œæˆ |

### ğŸŸ  P0.5 - é«˜ä¼˜å…ˆçº§ï¼ˆè´¨é‡ä¿éšœï¼‰

| åºå· | ä»»åŠ¡ | æ¥æº | è¯´æ˜ | é¢„è®¡å·¥æ—¶ | çŠ¶æ€ |
|------|------|------|------|----------|------|
| 4 | å•å…ƒæµ‹è¯•ï¼šv2.0 æ–°å¢æ¨¡å— | æµ‹è¯• | è¦†ç›– common/, worker/distributed/, worker/engines/ | 2-3 å¤© | ğŸ”„ è¿›è¡Œä¸­ |
| 5 | æµ‹è¯•è¦†ç›–ç‡è¾¾åˆ° 80%+ | æµ‹è¯• | ä½¿ç”¨ pytest-cov æ£€æµ‹ï¼Œè¡¥å……ç¼ºå¤±æµ‹è¯• | 1-2 å¤© | å¾…å¼€å§‹ |
| 6 | é›†æˆæµ‹è¯•ï¼šç«¯åˆ°ç«¯æµç¨‹ | æµ‹è¯• | Worker æ³¨å†Œâ†’ä»»åŠ¡æäº¤â†’æ¨ç†â†’ç»“æœè¿”å› | 2-3 å¤© | å¾…å¼€å§‹ |

### ğŸŸ¡ P1 - ä¸­ä¼˜å…ˆçº§ï¼ˆç”Ÿäº§åŒ–å‡†å¤‡ï¼‰

| åºå· | ä»»åŠ¡ | æ¥æº | è¯´æ˜ | é¢„è®¡å·¥æ—¶ | çŠ¶æ€ |
|------|------|------|------|----------|------|
| 7 | åŸºå‡†æµ‹è¯•ï¼šå•è¯·æ±‚å»¶è¿Ÿé™ä½ | Phase 5 | éªŒè¯æ¨æµ‹è§£ç æ•ˆæœï¼Œé‡åŒ–åŠ é€Ÿæ¯” | 1 å¤© | âœ… è„šæœ¬å·²å®Œæˆ |
| 8 | ç¼–å†™ç”Ÿäº§éƒ¨ç½²æ–‡æ¡£ | Phase 6 | åŒ…æ‹¬ Docker éƒ¨ç½²ã€K8s éƒ¨ç½²ã€é…ç½®è¯´æ˜ | 2-3 å¤© | å¾…å¼€å§‹ |
| 9 | æ€§èƒ½å‹æµ‹ä¸è°ƒä¼˜ | Phase 6 | è´Ÿè½½æµ‹è¯•ã€ç“¶é¢ˆåˆ†æã€å‚æ•°è°ƒä¼˜ | 3-5 å¤© | å¾…å¼€å§‹ |

### ğŸŸ¢ P2 - ä½ä¼˜å…ˆçº§ï¼ˆå¢å¼ºåŠŸèƒ½ï¼‰

| åºå· | ä»»åŠ¡ | æ¥æº | è¯´æ˜ | é¢„è®¡å·¥æ—¶ |
|------|------|------|------|----------|
| 10 | åˆ›å»º Grafana ç›‘æ§é¢æ¿ | Phase 6 | å¯è§†åŒ–ç›‘æ§ Dashboard | 1-2 å¤© |
| 11 | RDMA é›¶æ‹·è´ä¼˜åŒ– | Phase 3 | é«˜ç«¯åœºæ™¯ KV-Cache ä¼ è¾“ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰ | 3-5 å¤© |

---

### ğŸ“‹ æµ‹è¯•è¦†ç›–è®¡åˆ’

**å·²æœ‰æµ‹è¯•æ–‡ä»¶** (`tests/`):
```
tests/
â”œâ”€â”€ conftest.py                         # æµ‹è¯•é…ç½®å’Œ fixtures
â”œâ”€â”€ _helpers_fs.py                      # æ–‡ä»¶ç³»ç»Ÿæµ‹è¯•åŠ©æ‰‹
â”œâ”€â”€ test_common_data_structures.py      # âœ… common æ¨¡å—æµ‹è¯•
â”œâ”€â”€ test_server_geo.py                  # âœ… åœ°ç†æœåŠ¡æµ‹è¯•
â”œâ”€â”€ test_server_privacy.py              # âœ… éšç§æœåŠ¡æµ‹è¯•
â”œâ”€â”€ test_worker_batch_processor.py      # âœ… æ‰¹å¤„ç†å™¨æµ‹è¯•
â”œâ”€â”€ test_worker_config.py               # âœ… é…ç½®æ¨¡å—æµ‹è¯•
â””â”€â”€ test_worker_distributed_session_exit.py  # âœ… åˆ†å¸ƒå¼ä¼šè¯æµ‹è¯•
```

**å¾…è¡¥å……æµ‹è¯•**:
| æ¨¡å— | æµ‹è¯•æ–‡ä»¶ | ä¼˜å…ˆçº§ | çŠ¶æ€ |
|------|----------|--------|------|
| `common/serialization.py` | `test_common_serialization.py` | P0.5 | ğŸ”„ è¿›è¡Œä¸­ |
| `worker/engines/llm_sglang.py` | `test_worker_engines_sglang.py` | P0.5 | å¾…å¼€å§‹ |
| `worker/engines/llm_vllm.py` | `test_worker_engines_vllm.py` | P0.5 | å¾…å¼€å§‹ |
| `worker/engines/speculative.py` | `test_worker_engines_speculative.py` | P0.5 | å¾…å¼€å§‹ |
| `worker/distributed/kv_cache.py` | `test_worker_distributed_kv_cache.py` | P0.5 | å¾…å¼€å§‹ |
| `worker/distributed/model_shard.py` | `test_worker_distributed_model_shard.py` | P0.5 | å¾…å¼€å§‹ |
| `worker/distributed/grpc_server.py` | `test_worker_distributed_grpc.py` | P1 | å¾…å¼€å§‹ |
| `server/app/services/pd_scheduler.py` | `test_server_pd_scheduler.py` | P0.5 | å¾…å¼€å§‹ |
| `server/app/services/observability.py` | `test_server_observability.py` | P1 | å¾…å¼€å§‹ |

---

### ğŸ“‹ ä»»åŠ¡ä¾èµ–å…³ç³»

```
P0 åŸºå‡†æµ‹è¯•ï¼ˆéªŒè¯åŠŸèƒ½ï¼‰
â”œâ”€â”€ [1] SGLang æ€§èƒ½æµ‹è¯• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”œâ”€â”€ [2] åˆ†å¸ƒå¼æ¨ç†æµ‹è¯• â”€â”€â”€â”€â”€â”€â”              â”‚
â”œâ”€â”€ [3] P/D åˆ†ç¦»æµ‹è¯• â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚
â””â”€â”€ [4] æ¨æµ‹è§£ç æµ‹è¯• â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â–º P1 å‹æµ‹è°ƒä¼˜ â”€â”´â”€â–º P2 ç›‘æ§é¢æ¿
                                    â”‚
                                    â–¼
                              P1 éƒ¨ç½²æ–‡æ¡£
```

---

### ğŸ¯ å»ºè®®æ‰§è¡Œé¡ºåº

**ç¬¬ä¸€é˜¶æ®µï¼ˆ1 å‘¨ï¼‰- åŠŸèƒ½éªŒè¯** âœ… è„šæœ¬å·²å®Œæˆ
```
1. [P0-1] SGLang æ€§èƒ½åŸºå‡†æµ‹è¯• âœ…
   - å·²åˆ›å»º benchmarks/single_worker.py
   - å¯¹æ¯” Transformers vs SGLang vs vLLM
   - æµ‹è¯•æŒ‡æ ‡ï¼šå»¶è¿Ÿã€ååé‡ã€æ˜¾å­˜å ç”¨

2. [P0-2] åˆ†å¸ƒå¼æ¨ç†åŸºå‡†æµ‹è¯• âœ…
   - å·²åˆ›å»º benchmarks/distributed.py
   - æµ‹è¯• Llama-70B è·¨ 3 å¡æ¨ç†ï¼ˆæ¨¡æ‹Ÿ/çœŸå®æ¨¡å¼ï¼‰
   - æµ‹è¯•æŒ‡æ ‡ï¼šç«¯åˆ°ç«¯å»¶è¿Ÿã€KV-Cache ä¼ è¾“å¼€é”€
```

**ç¬¬äºŒé˜¶æ®µï¼ˆ1 å‘¨ï¼‰- æ€§èƒ½éªŒè¯** âœ… è„šæœ¬å·²å®Œæˆ
```
3. [P0-3] Prefill/Decode åˆ†ç¦»æµ‹è¯• âœ…
   - å·²åˆ›å»º benchmarks/pd_separation.py
   - æµ‹è¯• TTFTï¼ˆé¦– Token æ—¶é—´ï¼‰æå‡
   - æµ‹è¯•ååé‡ï¼ˆTokens/sï¼‰æå‡
   - å¯¹æ¯”åˆ†ç¦» vs æ··åˆæ¨¡å¼

4. [P1-4] æ¨æµ‹è§£ç æµ‹è¯• âœ…
   - å·²åˆ›å»º benchmarks/speculative.py
   - æµ‹è¯•å•è¯·æ±‚å»¶è¿Ÿé™ä½
   - æµ‹è¯•æ¥å—ç‡ä¸åŠ é€Ÿæ¯”
```

**ç¬¬ä¸‰é˜¶æ®µï¼ˆ1-2 å‘¨ï¼‰- ç”Ÿäº§åŒ–**
```
5. [P1-5] ç¼–å†™éƒ¨ç½²æ–‡æ¡£
   - Docker Compose éƒ¨ç½²æŒ‡å—
   - Kubernetes éƒ¨ç½²æŒ‡å—
   - é…ç½®æœ€ä½³å®è·µ

6. [P1-6] æ€§èƒ½å‹æµ‹ä¸è°ƒä¼˜
   - ä½¿ç”¨ locust/k6 è¿›è¡Œè´Ÿè½½æµ‹è¯•
   - åˆ†æç“¶é¢ˆï¼ˆCPU/GPU/ç½‘ç»œ/å†…å­˜ï¼‰
   - è°ƒä¼˜æ‰¹å¤„ç†å‚æ•°ã€ç¼“å­˜å¤§å°ç­‰
```

**ç¬¬å››é˜¶æ®µï¼ˆå¯é€‰ï¼‰- å¢å¼º**
```
7. [P2-7] Grafana ç›‘æ§é¢æ¿
   - å¯¼å…¥ Prometheus æ•°æ®æº
   - åˆ›å»ºæ¨ç†å»¶è¿Ÿã€ååé‡ã€GPU ä½¿ç”¨ç‡é¢æ¿

8. [P2-8] RDMA é›¶æ‹·è´ä¼˜åŒ–
   - ä»…åœ¨å…·å¤‡ InfiniBand ç½‘ç»œæ—¶å®æ–½
   - å‚è€ƒ Mooncake/FlowKV å®ç°
```

---

### ğŸ“ åŸºå‡†æµ‹è¯•è„šæœ¬ (benchmarks/)

**å·²åˆ›å»ºçš„åŸºå‡†æµ‹è¯•è„šæœ¬**ï¼š
```
benchmarks/
â”œâ”€â”€ single_worker.py     # P0-1: å• Worker æ¨ç†æ€§èƒ½æµ‹è¯• (SGLang vs vLLM vs Transformers)
â”œâ”€â”€ distributed.py       # P0-2: åˆ†å¸ƒå¼æ¨ç†æµ‹è¯• (æ¨¡æ‹Ÿ/çœŸå®æ¨¡å¼)
â”œâ”€â”€ pd_separation.py     # P0-3: Prefill/Decode åˆ†ç¦»æµ‹è¯• (TTFT/ååé‡)
â””â”€â”€ speculative.py       # P1-4: æ¨æµ‹è§£ç æµ‹è¯• (EAGLE-3 é£æ ¼)
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
# å• Worker æ€§èƒ½æµ‹è¯•
python benchmarks/single_worker.py --backend all --model Qwen/Qwen2.5-7B-Instruct

# åˆ†å¸ƒå¼æ¨ç†æµ‹è¯•ï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰
python benchmarks/distributed.py --mode simulate --workers 3

# P/D åˆ†ç¦»å¯¹æ¯”æµ‹è¯•
python benchmarks/pd_separation.py --compare

# æ¨æµ‹è§£ç å¯¹æ¯”æµ‹è¯•
python benchmarks/speculative.py --compare
```

---

### ğŸ“Š è¿›åº¦è¿½è¸ª

| é˜¶æ®µ | å·²å®Œæˆ | å¾…å®Œæˆ | å®Œæˆç‡ |
|------|--------|--------|--------|
| Phase 1 | 4 | 0 | 100% âœ… |
| Phase 2 | 6 | 0 | 100% âœ… |
| Phase 3 | 5 | 0 | 100% âœ… |
| Phase 4 | 5 | 0 | 100% âœ… |
| Phase 5 | 5 | 0 | 100% âœ… |
| Phase 6 | 2 | 3 | 40% |
| åŸºå‡†æµ‹è¯•è„šæœ¬ | 4 | 0 | 100% âœ… |
| å•å…ƒæµ‹è¯• | è¿›è¡Œä¸­ | - | ğŸ”„ |
| **æ€»è®¡** | **31** | **3** | **91%** |

---

*æ–‡æ¡£ç”Ÿæˆæ—¶é—´ï¼š2025-12-30*
*åŸºäº Petalsã€vLLMã€SGLangã€DistServeã€Mooncake ç­‰é¡¹ç›®åˆ†æ*
